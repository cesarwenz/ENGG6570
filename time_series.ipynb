{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb6c8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from numpy.random import randn\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import time as t\n",
    "from statistics import mean\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c0e89f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10240)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dec5feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discriminator model\n",
    "def define_discriminator(image_shape, vector_shape):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # source image input\n",
    "    in_src_image = Input(shape=image_shape)\n",
    "    # target image input\n",
    "    in_target = Input(shape=(vector_shape[0], vector_shape[1]))\n",
    "    # Vector layer\n",
    "    n_nodes = (image_shape[0] * image_shape[1] * image_shape[2])\n",
    "    # LSTM Model\n",
    "    p = LSTM(200, activation='relu')(in_target)\n",
    "    #p = LSTM(200, activation=None)(in_target)\n",
    "    p = Dense(n_nodes)(p)\n",
    "    # Reshape and mergo to image dimension\n",
    "    p = Reshape((image_shape[0], image_shape[1], image_shape[2]))(p)\n",
    "    merged = Concatenate()([p, in_src_image])\n",
    "    # concatenate images channel-wise\n",
    "    # C64\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C128\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C256\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C512\n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # second last output layer\n",
    "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # patch output\n",
    "    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Flatten()(d)\n",
    "    d = Dropout(0.4)(d)\n",
    "    out_layer = Dense(1, activation='sigmoid')(d)\n",
    "    # define model\n",
    "    model = Model([in_src_image, in_target], out_layer)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a67697c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an encoder block\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # add downsampling layer\n",
    "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same',\n",
    "    kernel_initializer=init)(layer_in)\n",
    "    # conditionally add batch normalization\n",
    "    if batchnorm:\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "    # leaky relu activation\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebf4df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(in_shape, vector_shape, latent_dim):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    in_lat = Input(shape=latent_dim)\n",
    "    gen = LeakyReLU(alpha=0.2)(in_lat)\n",
    "    # merge image gen and label input\n",
    "    # encoder model\n",
    "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "    e2 = define_encoder_block(e1, 128)\n",
    "    e3 = define_encoder_block(e2, 256)\n",
    "    e4 = define_encoder_block(e3, 512)\n",
    "    e5 = define_encoder_block(e4, 512)\n",
    "    e6 = define_encoder_block(e5, 512)\n",
    "    e7 = define_encoder_block(e6, 512)\n",
    "    # bottleneck, no batch norm and relu\n",
    "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "    b = Activation('relu')(b)\n",
    "    b = Flatten()(b)\n",
    "    # Merge latent space variable input with network\n",
    "    b = Concatenate()([b, gen])\n",
    "    # Reshape to vector size\n",
    "    n_nodes = (vector_shape[0] * vector_shape[1])\n",
    "    b = Dense(n_nodes)(b)\n",
    "    b = Reshape([vector_shape[0], vector_shape[1]])(b)\n",
    "    # encoder-decoder LSTM model\n",
    "    d = LSTM(200, activation='relu')(b)\n",
    "    d = RepeatVector(vector_shape[0])(d)\n",
    "    d = LSTM(200, activation='relu', return_sequences=True)(d)\n",
    "    # output\n",
    "    out_layer = Dense(3)(d)\n",
    "\n",
    "    # define model input & output\n",
    "    model = Model([in_image, in_lat], out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35c08fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model, image_shape, vector_shape, latent_dim):\n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in d_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    # define the source image\n",
    "    in_src = Input(shape=image_shape)\n",
    "    in_lat = Input(shape=latent_dim)\n",
    "    # connect the source image to the generator input\n",
    "    gen_out = g_model([in_src, in_lat])\n",
    "    # connect the source input and generator output to the discriminator input\n",
    "    dis_out = d_model([in_src, gen_out])\n",
    "    # src image as input, generated image and classification output\n",
    "    model = Model([in_src, in_lat], [dis_out, gen_out])\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51c3ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare training images\n",
    "def load_real_samples(filename):\n",
    "\t# load compressed arrays\n",
    "\tdata = load(filename)\n",
    "\t# unpack arrays\n",
    "\tX1, X2 = data['arr_0'], data['arr_1']\n",
    "\treturn [X1, X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "32300347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, trainA.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX1, X2 = trainA[ix], trainB[ix]\n",
    "\t# generate 'real' class labels \n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn [X1, X2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "daed564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff4a55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, n_samples):\n",
    "    # Generate latent space points\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # generate fake instance\n",
    "    X = g_model.predict([samples, z_input])\n",
    "    # create 'fake' class labels \n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30713502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay trajectories (data_y) to the image (data_x)\n",
    "def create_trajectory(data_x, data_y, obs_len=10):\n",
    "    # Calibration parameter to overlay for a 1280x360 resolution image\n",
    "    K = np.array([[537.023764, 0, 640 , 0], \n",
    "                    [0 , 537.023764, 180, 0], \n",
    "                    [0, 0, 1, 0]])\n",
    "    # Rotation matrix to obtain egocentric trajectory\n",
    "    Rt = np.array([[0.028841, 0.007189, 0.999558, 1.481009],\n",
    "                    [-0.999575,  0.004514,  0.028809,  0.296583],\n",
    "                    [ 0.004305,  0.999964, -0.007316, -1.544537],\n",
    "                    [ 0.      ,  0.      ,  0.      ,  1.      ]])\n",
    "\n",
    "    # Resize data back to 1280x360\n",
    "    data_x = cv2.resize(data_x, (1280,360))\n",
    "    # Add column of ones for rotation matrix multiplication\n",
    "    data_y = np.hstack((data_y, np.ones((len(data_y),1))))\n",
    "    # Draw points\n",
    "    for m in range(obs_len, data_y.shape[0]):\n",
    "        # Rotation matrix multiplication of trajectory \n",
    "        A = np.matmul(np.linalg.inv(Rt), data_y[m, :].reshape(4, 1))\n",
    "        # Egocentric view of trajectory\n",
    "        B = np.matmul(K, A)\n",
    "        # Circle location of trajectories \n",
    "        x = int(B[0, 0] * 1.0 / B[2, 0])\n",
    "        y = int(B[1, 0] * 1.0 / B[2, 0])\n",
    "        if (x < 0 or x > 1280 - 1 or y > 360 - 1):\n",
    "            continue\n",
    "        # Use opencv to overlay trajectories\n",
    "        data_x = cv2.circle(data_x, (x, y), 3, (0, 0, 255), -1)\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "169a6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, dataset, n_samples=1):\n",
    "    # select a sample of input images\n",
    "    [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples)\n",
    "    # generate a batch of fake samples\n",
    "    X_fakeB, _ = generate_fake_samples(g_model, X_realA, n_samples)\n",
    "    # scale all pixels from [-1,1] to [0,1]\n",
    "    X_realA = (X_realA + 1) / 2.0\n",
    "    # plot real source images\n",
    "    for i in range(n_samples):\n",
    "        orig_image = (X_realA[i]* 255).astype(np.uint8)\n",
    "        orig_image = cv2.resize(orig_image, (1280,360))\n",
    "        pyplot.subplot(3, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(orig_image)\n",
    "    # plot generated target image\n",
    "    for i in range(n_samples):\n",
    "        fake_sample = create_trajectory((X_realA[i]* 255).astype(np.uint8), X_fakeB[i])\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(fake_sample)\n",
    "    # plot real target image\n",
    "    for i in range(n_samples):\n",
    "        true_sample = create_trajectory((X_realA[i]* 255).astype(np.uint8), X_realB[i])\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(true_sample)\n",
    "    # save plot to file\n",
    "    filename1 = 'plot_%06d.png' % (step+1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    # save the generator model\n",
    "    filename2 = 'model_%06d.h5' % (step+1)\n",
    "    g_model.save(filename2)\n",
    "    print('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca947f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "def train(d_model, g_model, gan_model, dataset, latent_dim, n_epochs=50, n_batch=1):\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(len(dataset[0]) / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    #n_steps = bat_per_epo * n_epochs\n",
    "    #for timining\n",
    "    n_steps = 25\n",
    "    times = []\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        #start timer\n",
    "        start = t.time()\n",
    "        # select a batch of real samples\n",
    "        [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch)\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_batch)\n",
    "        # update discriminator for real samples\n",
    "        d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "        # update discriminator for generated samples\n",
    "        d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "        X_lat = generate_latent_points(latent_dim, n_batch)\n",
    "        # update the generator\n",
    "        g_loss, _, _ = gan_model.train_on_batch([X_realA, X_lat], [y_real, X_realB])\n",
    "        #end timer\n",
    "        end = t.time()\n",
    "        #print(f'time for itr: {end-start} sec')\n",
    "        times.append(end-start)\n",
    "        # summarize performance\n",
    "        print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "        # summarize model performance\n",
    "        if (i+1) % int(bat_per_epo) == 0:\n",
    "            summarize_performance(i, g_model, dataset)\n",
    "            print('saved')\n",
    "    print(f'average time per itr: {mean(times)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d364cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (5866, 256, 256, 3) (5866, 40, 3)\n",
      "CPU times: user 8.31 s, sys: 339 ms, total: 8.65 s\n",
      "Wall time: 8.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load image data\n",
    "dataset = load_real_samples('0000.npz')\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3ce285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "(40, 3)\n"
     ]
    }
   ],
   "source": [
    "# define input shape based on the loaded dataset\n",
    "image_shape = dataset[0].shape[1:]\n",
    "vector_shape = dataset[1].shape[1:]\n",
    "latent_dim = 512\n",
    "print(image_shape)\n",
    "print(vector_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9e673f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# define the models\n",
    "d_model = define_discriminator(image_shape, vector_shape)\n",
    "g_model = define_generator(image_shape, vector_shape, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6043d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the composite model\n",
    "gan_model = define_gan(g_model, d_model, image_shape, vector_shape, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9b297e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1[0.612] d2[2.692] g[601.811]\n",
      ">2, d1[0.031] d2[0.684] g[483.340]\n",
      ">3, d1[0.000] d2[0.743] g[718.046]\n",
      ">4, d1[0.398] d2[0.859] g[65.944]\n",
      ">5, d1[0.233] d2[0.518] g[82.864]\n",
      ">6, d1[0.021] d2[0.320] g[638.225]\n",
      ">7, d1[0.749] d2[0.562] g[80.224]\n",
      ">8, d1[0.519] d2[0.688] g[10.784]\n",
      ">9, d1[0.844] d2[0.405] g[8.180]\n",
      ">10, d1[0.199] d2[0.241] g[325.617]\n",
      ">11, d1[0.021] d2[0.149] g[619.207]\n",
      ">12, d1[0.169] d2[0.203] g[391.178]\n",
      ">13, d1[0.000] d2[0.451] g[561.596]\n",
      ">14, d1[0.013] d2[0.254] g[329.133]\n",
      ">15, d1[0.094] d2[0.233] g[267.052]\n",
      ">16, d1[0.000] d2[0.207] g[540.813]\n",
      ">17, d1[0.137] d2[0.481] g[172.904]\n",
      ">18, d1[0.000] d2[0.341] g[391.129]\n",
      ">19, d1[0.000] d2[0.358] g[427.875]\n",
      ">20, d1[0.000] d2[0.090] g[640.850]\n",
      ">21, d1[0.065] d2[0.288] g[139.335]\n",
      ">22, d1[0.384] d2[0.481] g[31.991]\n",
      ">23, d1[0.700] d2[0.427] g[30.135]\n",
      ">24, d1[0.000] d2[0.714] g[671.657]\n",
      ">25, d1[0.000] d2[0.186] g[606.114]\n",
      "average time per itr: 0.24941622734069824\n"
     ]
    }
   ],
   "source": [
    "train(d_model, g_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d5935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5b091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4bee8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd096fc4c12eed3e18ead224912196259d9b4ba259ad818a335a1ec786e41f96ad8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
