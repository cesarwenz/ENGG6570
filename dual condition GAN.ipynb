{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bb8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from numpy.random import randn\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7452b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 11GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10240)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec8104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discriminator model\n",
    "def define_discriminator(image_shape, vector_shape, n_classes=7):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # source image input\n",
    "    in_src_image = Input(shape=image_shape)\n",
    "    # vector input\n",
    "    in_target = Input(shape=(vector_shape[0], vector_shape[1]))\n",
    "    # label input\n",
    "    in_label = Input(shape=(1,))\n",
    "    # Vector layer\n",
    "    n_nodes = (image_shape[0] * image_shape[1] * image_shape[2])\n",
    "    # embedding for categorical input\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    # scale up to image dimensions with linear\n",
    "    li = Dense(n_nodes)(li)\n",
    "    # reshape to additional channel\n",
    "    li = Reshape((image_shape[0], image_shape[1], image_shape[2]))(li)\n",
    "    # LSTM Model\n",
    "    p = LSTM(200, activation='relu')(in_target)\n",
    "    p = Dense(n_nodes)(p)\n",
    "    # Reshape and mergo to image dimension\n",
    "    p = Reshape((image_shape[0], image_shape[1], image_shape[2]))(p)\n",
    "    merged = Concatenate()([p, in_src_image, li])\n",
    "    # concatenate images channel-wise\n",
    "    # C64\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C128\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C256\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C512\n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # second last output layer\n",
    "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # patch output\n",
    "    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Flatten()(d)\n",
    "    d = Dropout(0.4)(d)\n",
    "    out_layer = Dense(1, activation='sigmoid')(d)\n",
    "    # define model\n",
    "    model = Model([in_src_image, in_target, in_label], out_layer)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ff581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an encoder block\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # add downsampling layer\n",
    "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same',\n",
    "    kernel_initializer=init)(layer_in)\n",
    "    # conditionally add batch normalization\n",
    "    if batchnorm:\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "    # leaky relu activation\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ee8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(in_shape, vector_shape, latent_dim, n_classes=7):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    in_lat = Input(shape=latent_dim)\n",
    "    in_label = Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    li = Dense(latent_dim)(li)\n",
    "    li = Reshape([latent_dim])(li)\n",
    "    gen = LeakyReLU(alpha=0.2)(in_lat)\n",
    "    # merge image gen and label input\n",
    "    # encoder model\n",
    "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "    e2 = define_encoder_block(e1, 128)\n",
    "    e3 = define_encoder_block(e2, 256)\n",
    "    e4 = define_encoder_block(e3, 512)\n",
    "    e5 = define_encoder_block(e4, 512)\n",
    "    e6 = define_encoder_block(e5, 512)\n",
    "    e7 = define_encoder_block(e6, 512)\n",
    "    # bottleneck, no batch norm and relu\n",
    "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "    b = Activation('relu')(b)\n",
    "    b = Flatten()(b)\n",
    "    # Merge latent space variable input with network\n",
    "    b = Concatenate()([b, gen, li])\n",
    "    # Reshape to vector size\n",
    "    n_nodes = (vector_shape[0] * vector_shape[1])\n",
    "    b = Dense(n_nodes)(b)\n",
    "    b = Reshape([vector_shape[0], vector_shape[1]])(b)\n",
    "    # encoder-decoder LSTM model\n",
    "    d = LSTM(200, activation='relu')(b)\n",
    "    d = RepeatVector(vector_shape[0])(d)\n",
    "    d = LSTM(200, activation='relu', return_sequences=True)(d)\n",
    "    # output\n",
    "    out_layer = Dense(3)(d)\n",
    "\n",
    "    # define model input & output\n",
    "    model = Model([in_image, in_lat, in_label], out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd58e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model, image_shape, vector_shape, latent_dim):\n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in d_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    # define the source image\n",
    "    in_src = Input(shape=image_shape)\n",
    "    in_lat = Input(shape=latent_dim)\n",
    "    in_label = Input(shape=(1,))\n",
    "    # connect the source image to the generator input\n",
    "    gen_out = g_model([in_src, in_lat, in_label])\n",
    "    # connect the source input and generator output to the discriminator input\n",
    "    dis_out = d_model([in_src, gen_out, in_label])\n",
    "    # src image as input, generated image and classification output\n",
    "    model = Model([in_src, in_lat, in_label], [dis_out, gen_out])\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e283c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare training images\n",
    "def load_real_samples(filename):\n",
    "\t# load compressed arrays\n",
    "\tdata = load(filename)\n",
    "\t# unpack arrays\n",
    "\tX1, X2, X3 = data['arr_0'], data['arr_1'], data['arr_2']\n",
    "\treturn [X1, X2, X3-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c1479f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# unpack dataset\n",
    "\timg, vec, label = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, img.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX1, X2, X3 = img[ix], vec[ix], label[ix]\n",
    "\t# generate 'real' class labels \n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn [X1, X2, X3], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52bccb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ac0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, label, n_samples):\n",
    "    # Generate latent space points\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # generate fake instance\n",
    "    X = g_model.predict([samples, z_input, label])\n",
    "    # create 'fake' class labels \n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdfebda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay trajectories (data_y) to the image (data_x)\n",
    "def create_trajectory(data_x, data_y, obs_len=10):\n",
    "    # Calibration parameter to overlay for a 1280x360 resolution image\n",
    "    K = np.array([[537.023764, 0, 640 , 0], \n",
    "                    [0 , 537.023764, 180, 0], \n",
    "                    [0, 0, 1, 0]])\n",
    "    # Rotation matrix to obtain egocentric trajectory\n",
    "    Rt = np.array([[0.028841, 0.007189, 0.999558, 1.481009],\n",
    "                    [-0.999575,  0.004514,  0.028809,  0.296583],\n",
    "                    [ 0.004305,  0.999964, -0.007316, -1.544537],\n",
    "                    [ 0.      ,  0.      ,  0.      ,  1.      ]])\n",
    "\n",
    "    # Resize data back to 1280x360\n",
    "    data_x = cv2.resize(data_x, (1280,360))\n",
    "    # Add column of ones for rotation matrix multiplication\n",
    "    data_y = np.hstack((data_y, np.ones((len(data_y),1))))\n",
    "    # Draw points\n",
    "    for m in range(obs_len, data_y.shape[0]):\n",
    "        # Rotation matrix multiplication of trajectory \n",
    "        A = np.matmul(np.linalg.inv(Rt), data_y[m, :].reshape(4, 1))\n",
    "        # Egocentric view of trajectory\n",
    "        B = np.matmul(K, A)\n",
    "        # Circle location of trajectories \n",
    "        x = int(B[0, 0] * 1.0 / B[2, 0])\n",
    "        y = int(B[1, 0] * 1.0 / B[2, 0])\n",
    "        if (x < 0 or x > 1280 - 1 or y > 360 - 1):\n",
    "            continue\n",
    "        # Use opencv to overlay trajectories\n",
    "        data_x = cv2.circle(data_x, (x, y), 3, (0, 0, 255), -1)\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8f6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_drvact_text(drvact):\n",
    "    text = '[warnings] drvact label is not defined ...'\n",
    "    if (drvact == 1):\n",
    "        text = 'Go'\n",
    "    elif (drvact == 2):\n",
    "        text = 'Turn Left'\n",
    "    elif (drvact == 3):\n",
    "        text = 'Turn Right'\n",
    "    elif (drvact == 4):\n",
    "        text = 'U-turn'\n",
    "    elif (drvact == 5):\n",
    "        text = 'Left LC'\n",
    "    elif (drvact == 6):\n",
    "        text = 'Right LC'\n",
    "    elif (drvact == 7):\n",
    "        text = 'Avoidance'\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c71f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, dataset, n_samples=1):\n",
    "    # select a sample of input images\n",
    "    [X_real_img, X_real_vec, X_label], _ = generate_real_samples(dataset, n_samples)\n",
    "    # generate a batch of fake samples\n",
    "    X_fake_vec, _ = generate_fake_samples(g_model, X_real_img, X_label, n_samples)\n",
    "    # scale all pixels from [-1,1] to [0,1]\n",
    "    X_real_img = (X_real_img + 1) / 2.0\n",
    "    pyplot.figure(figsize=(32.0, 20.0))\n",
    "    # plot real source images\n",
    "    for i in range(n_samples):\n",
    "        orig_image = (X_real_img[i]* 255).astype(np.uint8)\n",
    "        orig_image = cv2.resize(orig_image, (1280,360))\n",
    "        pyplot.subplot(3, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(orig_image)\n",
    "    # plot generated target image\n",
    "    for i in range(n_samples):\n",
    "        fake_sample = create_trajectory((X_real_img[i]* 255).astype(np.uint8), X_fake_vec[i])\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(fake_sample)\n",
    "    # plot real target image\n",
    "    for i in range(n_samples):\n",
    "        true_sample = create_trajectory((X_real_img[i]* 255).astype(np.uint8), X_real_vec[i])\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(true_sample)\n",
    "    # save plot to file\n",
    "    filename1 = 'plot_%06d.png' % (step+1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    # save the generator model\n",
    "    filename2 = 'model_%06d.h5' % (step+1)\n",
    "    g_model.save(filename2)\n",
    "    filename3 = 'd_model_%06d.h5' % (step+1)\n",
    "    d_model.save(filename3)\n",
    "    print('>Saved: %s, %s and %s' % (filename1, filename2, filename3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e1f30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "def train(d_model, g_model, gan_model, dataset, latent_dim, n_epochs=100, batch_size=30):\n",
    "    # iteration number to output checkpoint\n",
    "    display_iter = 5000\n",
    "    # calculate the number of batches\n",
    "    n_batch = int(len(dataset[0]) / batch_size)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = n_batch * n_epochs\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # select a batch of real samples\n",
    "        [X_real_img, X_real_vec, X_label], y_real = generate_real_samples(dataset, batch_size)\n",
    "        # generate a batch of fake samples\n",
    "        X_fake_vec, y_fake = generate_fake_samples(g_model, X_real_img, X_label, batch_size)\n",
    "        # update discriminator for real samples\n",
    "        d_loss1 = d_model.train_on_batch([X_real_img, X_real_vec, X_label], y_real)\n",
    "        # update discriminator for generated samples\n",
    "        d_loss2 = d_model.train_on_batch([X_real_img, X_fake_vec, X_label], y_fake)\n",
    "        X_lat = generate_latent_points(latent_dim, batch_size)\n",
    "        # update the generator\n",
    "        g_loss, _, _ = gan_model.train_on_batch([X_real_img, X_lat, X_label], [y_real, X_real_vec])\n",
    "        # summarize performance\n",
    "        print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "        # summarize model performance\n",
    "        if (i+1) % int(display_iter) == 0:\n",
    "            summarize_performance(i, g_model, dataset)\n",
    "            print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12bca134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (16805, 256, 256, 3) (16805, 40, 3) (16805,)\n",
      "CPU times: user 23 s, sys: 4.7 s, total: 27.7 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load image data\n",
    "dataset = load_real_samples('./dual_condition_dataset_train.npz')\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape, dataset[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4025fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# define input shape based on the loaded dataset\n",
    "image_shape = dataset[0].shape[1:]\n",
    "vector_shape = dataset[1].shape[1:]\n",
    "latent_dim = 512\n",
    "# %%\n",
    "# define the models\n",
    "d_model = define_discriminator(image_shape, vector_shape)\n",
    "g_model = define_generator(image_shape, vector_shape, latent_dim)\n",
    "# %%\n",
    "# define the composite model\n",
    "gan_model = define_gan(g_model, d_model, image_shape, vector_shape, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1[0.324] d2[1.158] g[419.046]\n",
      ">2, d1[0.063] d2[0.769] g[362.446]\n",
      ">3, d1[0.049] d2[0.683] g[429.082]\n",
      ">4, d1[0.103] d2[0.605] g[369.131]\n",
      ">5, d1[0.127] d2[0.452] g[344.344]\n",
      ">6, d1[0.162] d2[0.275] g[328.566]\n",
      ">7, d1[0.108] d2[0.255] g[389.623]\n",
      ">8, d1[0.059] d2[0.199] g[403.737]\n",
      ">9, d1[0.102] d2[0.195] g[376.501]\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train(d_model, g_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224677a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end the tensorflow session\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77247d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cuda to release GPU memory\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd096fc4c12eed3e18ead224912196259d9b4ba259ad818a335a1ec786e41f96ad8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
